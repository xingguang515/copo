import copy
from collections import defaultdict
from math import cos, sin

import numpy as np
from gym.spaces import Box, Dict
from metadrive.utils import get_np_random, clip
from ray.rllib.env import MultiAgentEnv
from ray.tune.registry import register_env

COMM_ACTIONS = "comm_actions"
COMM_PREV_ACTIONS = "comm_prev_actions"

# prev_obs_{t-1} is the concatenation of neighbors' message comm_action_{t-1}
COMM_PREV_OBS = "comm_prev_obs"

# current_obs_t is the concatenation of neighbors' message comm_action_{t-1}
COMM_CURRENT_OBS = "comm_current_obs"
COMM_PREV_2_OBS = "comm_prev_2_obs"

COMM_LOGITS = "comm_logits"
COMM_LOG_PROB = "comm_log_prob"
ENV_PREV_OBS = "env_prev_obs"

COMM_METHOD = "comm_method"

NEI_OBS = "nei_obs"


class DYEnv:
    """
    This class maintains a distance map of all agents and appends the
    neighbours' names and distances into info at each step.
    We should subclass this class to a base environment class.
    """
    @classmethod
    def default_config(cls):
        config = super(DYEnv, cls).default_config()
        # Note that this config is set to 40 in LCFEnv
        config["neighbours_distance"] = 40

        config.update(
            dict(
                communication=dict(comm_method="none", comm_size=4, comm_neighbours=4, add_pos_in_comm=False),
                add_traffic_light=False,
                traffic_light_interval=30,
            )
        )

        return config

    def __init__(self, *args, **kwargs):
        self.distance_map = defaultdict(lambda: defaultdict(lambda: float("inf")))
        super(DYEnv, self).__init__(*args, **kwargs)


    def _get_reset_return(self):
        if self.config["communication"][COMM_METHOD] != "none":
            self._comm_obs_buffer = defaultdict()
        return super(DYEnv, self)._get_reset_return()

    @property
    def action_space(self):
        old_action_space = super(DYEnv, self).action_space
        if not self.config["communication"][COMM_METHOD] != "none":
            return old_action_space
        assert isinstance(old_action_space, Dict)
        new_action_space = Dict(
            {
                k: Box(
                    low=single.low[0],
                    high=single.high[0],
                    dtype=single.dtype,

                    # We are not using self._comm_dim here!
                    shape=(single.shape[0] + self.config["communication"]["comm_size"], )
                )
                for k, single in old_action_space.spaces.items()
            }
        )
        return new_action_space

    def step(self, actions):

        if self.config["communication"][COMM_METHOD] != "none":
            comm_actions = {k: v[2:] for k, v in actions.items()}
            actions = {k: v[:2] for k, v in actions.items()}

        o, r, d, i = super(DYEnv, self).step(actions)
        self._update_distance_map(dones=d)
        for kkk in i.keys():
            i[kkk]["all_agents"] = list(i.keys())

            neighbours, nei_distances = self._find_in_range(kkk, self.config["neighbours_distance"])
            i[kkk]["neighbours"] = neighbours
            i[kkk]["neighbours_distance"] = nei_distances

            if self.config["communication"][COMM_METHOD] != "none":
                i[kkk][COMM_CURRENT_OBS] = []
                for n in neighbours[:self.config["communication"]["comm_neighbours"]]:
                    if n in comm_actions:
                        if self.config["communication"]["add_pos_in_comm"]:
                            ego_vehicle = self.vehicles_including_just_terminated[kkk]
                            nei_vehicle = self.vehicles_including_just_terminated[n]
                            relative_position = ego_vehicle.projection(nei_vehicle.position - ego_vehicle.position)
                            dis = np.linalg.norm(relative_position)
                            extra_comm_obs = [
                                dis / 20, ((relative_position[0] / dis) + 1) / 2, ((relative_position[1] / dis) + 1) / 2
                            ]
                            tmp_comm_obs = np.concatenate([comm_actions[n], np.clip(np.asarray(extra_comm_obs), 0, 1)])
                        else:
                            tmp_comm_obs = comm_actions[n]
                        i[kkk][COMM_CURRENT_OBS].append(tmp_comm_obs)
                    else:
                        i[kkk][COMM_CURRENT_OBS].append(np.zeros((self._comm_dim, )))

        return o, r, d, i

    def _find_in_range(self, v_id, distance):
        if distance <= 0:
            return [], []
        max_distance = distance
        dist_to_others = self.distance_map[v_id]
        dist_to_others_list = sorted(dist_to_others, key=lambda k: dist_to_others[k])
        ret = [
            dist_to_others_list[i] for i in range(len(dist_to_others_list))
            if dist_to_others[dist_to_others_list[i]] < max_distance
        ]
        ret2 = [
            dist_to_others[dist_to_others_list[i]] for i in range(len(dist_to_others_list))
            if dist_to_others[dist_to_others_list[i]] < max_distance
        ]
        return ret, ret2

    def _update_distance_map(self, dones=None):
        self.distance_map.clear()
        if hasattr(self, "vehicles_including_just_terminated"):
            vehicles = self.vehicles_including_just_terminated
            # if dones is not None:
            #     assert (set(dones.keys()) - set(["__all__"])) == set(vehicles.keys()), (dones, vehicles)
        else:
            vehicles = self.vehicles  # Fallback to old version MetaDrive, but this is not accurate!
        keys = [k for k, v in vehicles.items() if v is not None]
        for c1 in range(0, len(keys) - 1):
            for c2 in range(c1 + 1, len(keys)):
                k1 = keys[c1]
                k2 = keys[c2]
                p1 = vehicles[k1].position
                p2 = vehicles[k2].position
                distance = np.linalg.norm(p1 - p2)
                self.distance_map[k1][k2] = distance
                self.distance_map[k2][k1] = distance


class LCFEnv(DYEnv):
    @classmethod
    def default_config(cls):
        config = super(LCFEnv, cls).default_config()
        config.update(
            dict(
                # Overwrite the CCEnv's neighbours_distance=10 to 40.
                neighbours_distance=40,

                # Two mode to compute utility for each vehicle:
                # "linear": util = r_me * lcf + r_other * (1 - lcf), lcf in [0, 1]
                # "angle": util = r_me * cos(lcf) + r_other * sin(lcf), lcf in [0, pi/2]
                # "angle" seems to be more stable!

                # If this is set to False, then the return reward is natively the LCF-weighted coordinated reward!
                # This will be helpful in ablation study!

            )
        )
        return config

    def __init__(self, config=None):
        super(LCFEnv, self).__init__(config)
        self.lcf_map = {}
        assert hasattr(super(LCFEnv, self), "_update_distance_map")
        self._traffic_light_counter = 0





    @property
    def _traffic_light_msg(self):
        fix_interval = self.config["traffic_light_interval"]
        increment = (self._traffic_light_counter % fix_interval) / fix_interval * 0.1
        if ((self._traffic_light_counter // fix_interval) % 2) == 1:
            return 0 + increment
        else:
            return 1 - increment

    def get_agent_traffic_light_msg(self, pos):
        b_box = self.engine.current_map.road_network.get_bounding_box()
        pos0 = (pos[0] - b_box[0]) / (b_box[1] - b_box[0])
        pos1 = (pos[1] - b_box[2]) / (b_box[3] - b_box[2])
        # print("Msg: {}, Pos0: {}, Pos1 {}".format(self._traffic_light_msg, pos0, pos1))
        return np.clip(np.array([self._traffic_light_msg, pos0, pos1]), 0, 1).astype(np.float32)



    def step(self, actions):
        # step the environment
        o, r, d, i = super(LCFEnv, self).step(actions)
        assert set(i.keys()) == set(o.keys())
        global_reward = sum(r.values()) / len(r.values())

        if self.config["add_traffic_light"]:
            self._traffic_light_counter += 1

        for agent_name, agent_info in i.items():
            assert "neighbours" in agent_info
            # Note: agent_info["neighbours"] records the neighbours within radius neighbours_distance.
            nei_rewards = [r[nei_name] for nei_name in agent_info["neighbours"]]
            if nei_rewards:
                i[agent_name]["nei_rewards"] = sum(nei_rewards) / len(nei_rewards)
            else:
                i[agent_name]["nei_rewards"] = 0.0  # Do not provide neighbour rewards if no neighbour
            i[agent_name]["global_rewards"] = global_reward

            if self.config["add_traffic_light"]:
                o[agent_name] = np.concatenate(
                    [
                        o[agent_name],
                        self.get_agent_traffic_light_msg(self.vehicles_including_just_terminated[agent_name].position)
                    ]
                )
        return o, r, d, i






def get_dyenv(env_class):
    name = env_class.__name__

    class TMP(DYEnv, env_class):
        pass

    TMP.__name__ = name
    TMP.__qualname__ = name
    return TMP





def get_lcf_env(env_class):
    name = env_class.__name__

    class TMP(LCFEnv, env_class):
        pass

    TMP.__name__ = name
    TMP.__qualname__ = name
    return TMP




def get_rllib_compatible_env(env_class, return_class=False):
    env_name = env_class.__name__

    class MA(env_class, MultiAgentEnv):
        _agent_ids = ["agent{}".format(i) for i in range(100)] + ["{}".format(i) for i in range(10000)] + ["sdc"]

        def __init__(self, config, *args, **kwargs):
            env_class.__init__(self, config, *args, **kwargs)
            MultiAgentEnv.__init__(self)

        @property
        def observation_space(self):
            ret = super(MA, self).observation_space
            if not hasattr(ret, "keys"):
                ret.keys = ret.spaces.keys
            return ret

        @property
        def action_space(self):
            ret = super(MA, self).action_space
            if not hasattr(ret, "keys"):
                ret.keys = ret.spaces.keys
            return ret

        def action_space_sample(self, agent_ids: list = None):
            """
            RLLib always has unnecessary stupid requirements that you have to bypass them by overwriting some
            useless functions.
            """
            return self.action_space.sample()

    MA.__name__ = env_name
    MA.__qualname__ = env_name
    register_env(env_name, lambda config: MA(config))

    if return_class:
        return env_name, MA

    return env_name


if __name__ == '__main__':
    # Test if the distance map is correctly updated.
    from metadrive.envs.marl_envs import MultiAgentIntersectionEnv
    from tqdm import trange

    env = get_lcf_env(MultiAgentIntersectionEnv)()
    env.reset(force_seed=0)

    for _ in trange(10):
        for _ in trange(1000):
            o, r, d, i = env.step({k: [0, 1] for k in env.vehicles})
            # print(d)
            print(env.vehicles_including_just_terminated)
            if d["__all__"]:
                env.reset()
                break

    env.close()